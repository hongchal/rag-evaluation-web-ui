"""Claude-based evaluation analysis service."""

from typing import List, Dict, Any
import structlog
from anthropic import Anthropic

from app.core.config import settings

logger = structlog.get_logger(__name__)


class ClaudeAnalysisService:
    """Service for generating AI-powered evaluation analysis using Claude."""
    
    def __init__(self):
        """Initialize Claude client."""
        api_key = settings.anthropic_api_key
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY environment variable not set")
        
        self.client = Anthropic(api_key=api_key)
        self.model = "claude-sonnet-4-5-20250929"  # Latest Claude model
    
    def analyze_evaluation_results(
        self,
        evaluation_name: str,
        metrics: List[Dict[str, Any]],
        dataset_name: str = None
    ) -> str:
        """
        Generate comprehensive analysis of evaluation results.
        
        Args:
            evaluation_name: Name of the evaluation
            metrics: List of pipeline metrics
            dataset_name: Optional dataset name
            
        Returns:
            Analysis text in markdown format
        """
        try:
            # Build context for Claude
            context = self._build_context(evaluation_name, metrics, dataset_name)
            
            # Create prompt
            prompt = f"""ë‹¹ì‹ ì€ RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì¢…í•©ì ì¸ ì˜ê²¬ì„ ì œê³µí•´ì£¼ì„¸ìš”:

{context}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

## ğŸ“ˆ ì „ì²´ ì„±ëŠ¥ ìš”ì•½
- ì „ë°˜ì ì¸ ê²€ìƒ‰ ì„±ëŠ¥ ìˆ˜ì¤€ í‰ê°€
- ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¸ íŒŒì´í”„ë¼ì¸ê³¼ ê·¸ ì´ìœ 

## ğŸ¯ íŒŒì´í”„ë¼ì¸ë³„ íŠ¹ì§•
- ê° íŒŒì´í”„ë¼ì¸ì˜ ê°•ì ê³¼ ì•½ì 
- ì§€í‘œë³„ ì„±ëŠ¥ ì°¨ì´ ë¶„ì„

## ğŸ’¡ ê°œì„  ì œì•ˆ
- ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ êµ¬ì²´ì ì¸ ì œì•ˆ
- ê° íŒŒì´í”„ë¼ì¸ì— ë§ëŠ” ìµœì í™” ë°©í–¥

## âš–ï¸ ì¢…í•© í‰ê°€
- ì „ì²´ì ì¸ í‰ê°€ì™€ ì¶”ì²œ íŒŒì´í”„ë¼ì¸
- ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë³„ ê¶Œì¥ì‚¬í•­

í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë˜, ê¸°ìˆ ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
            
            logger.info("requesting_claude_analysis", num_pipelines=len(metrics))
            
            # Call Claude API
            message = self.client.messages.create(
                model=self.model,
                max_tokens=2000,
                temperature=0.3,  # Lower temperature for more consistent analysis
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            # Extract text from response
            analysis = message.content[0].text
            
            logger.info("claude_analysis_completed", response_length=len(analysis))
            
            return analysis
            
        except Exception as e:
            logger.error("claude_analysis_failed", error=str(e))
            raise
    
    def _build_context(
        self,
        evaluation_name: str,
        metrics: List[Dict[str, Any]],
        dataset_name: str = None
    ) -> str:
        """Build context string for Claude."""
        lines = [
            f"**í‰ê°€ ì´ë¦„**: {evaluation_name}",
        ]
        
        if dataset_name:
            lines.append(f"**ë°ì´í„°ì…‹**: {dataset_name}")
        
        lines.append(f"**í‰ê°€ íŒŒì´í”„ë¼ì¸ ìˆ˜**: {len(metrics)}")
        lines.append("")
        lines.append("### íŒŒì´í”„ë¼ì¸ë³„ ì„±ëŠ¥ ì§€í‘œ:")
        lines.append("")
        
        for i, metric in enumerate(metrics, 1):
            pipeline_name = metric.get("pipeline_name", f"Pipeline #{metric.get('pipeline_id')}")
            lines.append(f"**{i}. {pipeline_name}**")
            lines.append(f"- NDCG@10: {metric.get('ndcg_at_k', 0) * 100:.1f}%")
            lines.append(f"- MRR (Mean Reciprocal Rank): {metric.get('mrr', 0) * 100:.1f}%")
            lines.append(f"- Recall@10: {metric.get('recall_at_k', 0) * 100:.1f}%")
            lines.append(f"- Precision@10: {metric.get('precision_at_k', 0) * 100:.1f}%")
            lines.append(f"- Hit Rate: {metric.get('hit_rate', 0) * 100:.1f}%")
            lines.append(f"- MAP (Mean Average Precision): {metric.get('map_score', 0) * 100:.1f}%")
            
            # Add timing info if available
            if metric.get('retrieval_time'):
                lines.append(f"- ê²€ìƒ‰ ì‹œê°„: {metric.get('retrieval_time'):.3f}ì´ˆ")
            if metric.get('chunking_time') and metric.get('chunking_time') > 0:
                lines.append(f"- ì²­í‚¹ ì‹œê°„: {metric.get('chunking_time'):.3f}ì´ˆ")
            if metric.get('embedding_time') and metric.get('embedding_time') > 0:
                lines.append(f"- ì„ë² ë”© ì‹œê°„: {metric.get('embedding_time'):.3f}ì´ˆ")
            
            if metric.get('num_chunks'):
                lines.append(f"- ì´ ì²­í¬ ìˆ˜: {metric.get('num_chunks')}")
            
            lines.append("")
        
        return "\n".join(lines)

