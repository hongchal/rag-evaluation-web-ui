#!/usr/bin/env python3
"""
ë²”ìš© ë°ì´í„°ì…‹ ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸

BEIR, Wikipedia, MS MARCO ë“± ë‹¤ì–‘í•œ RAG í‰ê°€ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ê³ 
í‰ê°€ ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

Usage:
    # BEIR ë°ì´í„°ì…‹
    python backend/scripts/prepare_dataset.py beir --name scifact
    python backend/scripts/prepare_dataset.py beir --name hotpotqa --sample 1000
    
    # ëª¨ë“  BEIR ë°ì´í„°ì…‹ í•œë²ˆì—
    python backend/scripts/prepare_dataset.py download-all
    
    # Wikipedia ë°ì´í„°ì…‹
    python backend/scripts/prepare_dataset.py wikipedia --size 1000
    
    # MS MARCO
    python backend/scripts/prepare_dataset.py msmarco --size 10000
    
    # ë°ì´í„°ì…‹ ê²€ì¦
    python backend/scripts/prepare_dataset.py verify --dataset backend/datasets/frames_eval.json
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional

# datasets ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from datasets import load_dataset
except ImportError:
    print("âŒ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install datasets")
    sys.exit(1)


def register_dataset_to_db(dataset_path: str) -> bool:
    """ë°ì´í„°ì…‹ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ë“±ë¡"""
    print(f"\nğŸ“ ë°ì´í„°ë² ì´ìŠ¤ì— ìë™ ë“±ë¡ ì¤‘...")
    try:
        # dataset_registry ëª¨ë“ˆ ì§ì ‘ ì‹¤í–‰
        import subprocess
        result = subprocess.run(
            [
                sys.executable,
                str(Path(__file__).parent / "dataset_registry.py"),
                "register",
                dataset_path
            ],
            env={**os.environ, "DATABASE_URL": os.environ.get("DATABASE_URL", "postgresql://postgres:postgres@localhost:5433/rag_evaluation")},
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            print(f"   âœ… ë°ì´í„°ë² ì´ìŠ¤ ë“±ë¡ ì™„ë£Œ!")
            return True
        else:
            print(f"   âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ë“±ë¡ ì‹¤íŒ¨")
            if result.stderr:
                print(f"   ì—ëŸ¬: {result.stderr[:200]}")
            print(f"   ìˆ˜ë™ìœ¼ë¡œ ë“±ë¡í•˜ë ¤ë©´:")
            print(f"   DATABASE_URL=\"postgresql://postgres:postgres@localhost:5433/rag_evaluation\" \\")
            print(f"     python3 backend/scripts/dataset_registry.py register {dataset_path}")
            return False
    except Exception as e:
        print(f"   âš ï¸  ìë™ ë“±ë¡ ì‹¤íŒ¨: {e}")
        print(f"   ìˆ˜ë™ìœ¼ë¡œ ë“±ë¡í•˜ë ¤ë©´:")
        print(f"   DATABASE_URL=\"postgresql://postgres:postgres@localhost:5433/rag_evaluation\" \\")
        print(f"     python3 backend/scripts/dataset_registry.py register {dataset_path}")
        return False


# BEIR ë°ì´í„°ì…‹ ëª©ë¡
BEIR_DATASETS = {
    "scifact": {
        "name": "BEIR SciFact",
        "description": "Scientific fact verification dataset",
        "size": "small"
    },
    "nfcorpus": {
        "name": "BEIR NFCorpus",
        "description": "Nutrition and health corpus",
        "size": "small"
    },
    "hotpotqa": {
        "name": "BEIR HotpotQA",
        "description": "Multi-hop question answering",
        "size": "large"
    },
    "fiqa": {
        "name": "BEIR FiQA",
        "description": "Financial question answering",
        "size": "medium"
    },
    "trec-covid": {
        "name": "BEIR TREC-COVID",
        "description": "COVID-19 research articles",
        "size": "large"
    }
}


class BEIRDatasetPreparer:
    """BEIR ë°ì´í„°ì…‹ ì¤€ë¹„"""
    
    def __init__(self, dataset_name: str, output_dir: str = "backend/datasets", sample_size: Optional[int] = None, auto_register: bool = True):
        self.dataset_name = dataset_name
        self.output_dir = Path(output_dir)
        self.sample_size = sample_size
        self.auto_register = auto_register
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def prepare(self) -> str:
        """BEIR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜"""
        
        if self.dataset_name not in BEIR_DATASETS:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” BEIR ë°ì´í„°ì…‹: {self.dataset_name}")
            print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹: {', '.join(BEIR_DATASETS.keys())}")
            return None
        
        info = BEIR_DATASETS[self.dataset_name]
        print(f"\nğŸ¦› BEIR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ: {info['name']}")
        print(f"   ì„¤ëª…: {info['description']}")
        print(f"   í¬ê¸°: {info['size']}")
        
        try:
            # BEIR ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
            from beir import util
            from beir.datasets.data_loader import GenericDataLoader
            
            # ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
            url = f"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{self.dataset_name}.zip"
            data_path = util.download_and_unzip(url, str(self.output_dir / ".beir"))
            
            # ë°ì´í„° ë¡œë“œ
            corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split="test")
            
            print(f"   âœ… ë¡œë“œ ì™„ë£Œ:")
            print(f"      - ë¬¸ì„œ: {len(corpus)}")
            print(f"      - ì¿¼ë¦¬: {len(queries)}")
            
            # ìƒ˜í”Œë§
            if self.sample_size:
                queries = dict(list(queries.items())[:self.sample_size])
                print(f"   ğŸ“Š ìƒ˜í”Œë§: {len(queries)} ì¿¼ë¦¬")
            
            # í‰ê°€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            documents = []
            eval_queries = []
            
            # ë¬¸ì„œ ë³€í™˜
            for doc_id, doc_data in corpus.items():
                documents.append({
                    "id": f"beir_{self.dataset_name}_{doc_id}",
                    "content": doc_data.get("text", ""),
                    "title": doc_data.get("title", "Untitled"),
                    "metadata": {
                        "source": f"beir_{self.dataset_name}",
                        "original_id": doc_id
                    }
                })
            
            # ì¿¼ë¦¬ ë³€í™˜
            for query_id, query_text in queries.items():
                # ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
                relevant_docs = []
                if query_id in qrels:
                    relevant_docs = [
                        f"beir_{self.dataset_name}_{doc_id}"
                        for doc_id in qrels[query_id].keys()
                    ]
                
                eval_queries.append({
                    "query": query_text,
                    "relevant_doc_ids": relevant_docs,
                    "expected_answer": "",  # BEIRëŠ” answer ì—†ìŒ
                    "difficulty": "medium",
                    "query_type": "retrieval",
                    "metadata": {
                        "source": f"beir_{self.dataset_name}",
                        "original_id": query_id
                    }
                })
            
            # ìµœì¢… ë°ì´í„°ì…‹
            evaluation_dataset = {
                "name": info["name"],
                "description": info["description"],
                "documents": documents,
                "queries": eval_queries,
                "metadata": {
                    "source": f"beir/{self.dataset_name}",
                    "total_documents": len(documents),
                    "total_queries": len(eval_queries)
                }
            }
            
            # ì €ì¥
            output_file = self.output_dir / f"beir_{self.dataset_name}_eval.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(evaluation_dataset, f, indent=2, ensure_ascii=False)
            
            file_size = output_file.stat().st_size / 1024 / 1024
            print(f"\nâœ… ì™„ë£Œ: {output_file}")
            print(f"   ğŸ“Š í†µê³„:")
            print(f"      - ë¬¸ì„œ: {len(documents)}")
            print(f"      - ì¿¼ë¦¬: {len(eval_queries)}")
            print(f"      - íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")
            
            # ìë™ DB ë“±ë¡
            if self.auto_register:
                self._register_to_db(str(output_file))
            
            return str(output_file)
            
        except ImportError:
            print("âŒ BEIR ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install beir")
            return None
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def _register_to_db(self, dataset_path: str):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ë“±ë¡"""
        register_dataset_to_db(dataset_path)


class WikipediaDatasetPreparer:
    """Wikipedia ë°ì´í„°ì…‹ ì¤€ë¹„"""
    
    def __init__(self, size: int = 1000, queries_per_doc: int = 2, output_dir: str = "backend/datasets", auto_register: bool = True):
        self.size = size
        self.queries_per_doc = queries_per_doc
        self.output_dir = Path(output_dir)
        self.auto_register = auto_register
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def prepare(self) -> str:
        """Wikipedia ë°ì´í„°ì…‹ ìƒì„±"""
        
        print(f"\nğŸ¦› Wikipedia ë°ì´í„°ì…‹ ìƒì„±")
        print(f"   ë¬¸ì„œ ìˆ˜: {self.size}")
        print(f"   ì¿¼ë¦¬/ë¬¸ì„œ: {self.queries_per_doc}")
        
        try:
            # Wikipedia ë°ì´í„°ì…‹ ë¡œë“œ
            print(f"\nğŸ“¦ Wikipedia ë°ì´í„°ì…‹ ë¡œë”©...")
            dataset = load_dataset("wikipedia", "20220301.en", split=f"train[:{self.size}]")
            print(f"   âœ… ë¡œë“œ ì™„ë£Œ: {len(dataset)} ë¬¸ì„œ")
            
            documents = []
            queries = []
            
            for idx, example in enumerate(dataset):
                if idx % 100 == 0:
                    print(f"   ì§„í–‰: {idx}/{len(dataset)}")
                
                title = example.get("title", "")
                text = example.get("text", "")
                
                if not text or len(text) < 100:
                    continue
                
                doc_id = f"wiki_{idx}"
                
                # ë¬¸ì„œ ìƒì„±
                documents.append({
                    "id": doc_id,
                    "content": text,
                    "title": title,
                    "metadata": {
                        "source": "wikipedia",
                        "doc_idx": idx
                    }
                })
                
                # ê°„ë‹¨í•œ ì¿¼ë¦¬ ìƒì„± (ì œëª© ê¸°ë°˜)
                for q_idx in range(self.queries_per_doc):
                    query_text = f"What is {title}?"
                    if q_idx == 1:
                        query_text = f"Tell me about {title}"
                    
                    queries.append({
                        "query": query_text,
                        "relevant_doc_ids": [doc_id],
                        "expected_answer": text[:200] + "...",
                        "difficulty": "easy",
                        "query_type": "factual",
                        "metadata": {
                            "source": "wikipedia",
                            "doc_idx": idx,
                            "query_idx": q_idx
                        }
                    })
            
            # ìµœì¢… ë°ì´í„°ì…‹
            evaluation_dataset = {
                "name": f"Wikipedia-{self.size}",
                "description": f"Wikipedia articles with auto-generated queries ({self.size} docs)",
                "documents": documents,
                "queries": queries,
                "metadata": {
                    "source": "wikipedia/20220301.en",
                    "total_documents": len(documents),
                    "total_queries": len(queries),
                    "queries_per_doc": self.queries_per_doc
                }
            }
            
            # ì €ì¥
            output_file = self.output_dir / f"wikipedia_{self.size}_eval.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(evaluation_dataset, f, indent=2, ensure_ascii=False)
            
            file_size = output_file.stat().st_size / 1024 / 1024
            print(f"\nâœ… ì™„ë£Œ: {output_file}")
            print(f"   ğŸ“Š í†µê³„:")
            print(f"      - ë¬¸ì„œ: {len(documents)}")
            print(f"      - ì¿¼ë¦¬: {len(queries)}")
            print(f"      - íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")
            
            # ìë™ DB ë“±ë¡
            if self.auto_register:
                register_dataset_to_db(str(output_file))
            
            return str(output_file)
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None


class MSMARCODatasetPreparer:
    """MS MARCO ë°ì´í„°ì…‹ ì¤€ë¹„"""
    
    def __init__(self, size: int = 10000, output_dir: str = "backend/datasets", auto_register: bool = True):
        self.size = size
        self.output_dir = Path(output_dir)
        self.auto_register = auto_register
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def prepare(self) -> str:
        """MS MARCO ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""
        
        print(f"\nğŸ¦› MS MARCO ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
        print(f"   í¬ê¸°: {self.size} passages")
        
        try:
            # MS MARCO ë°ì´í„°ì…‹ ë¡œë“œ
            print(f"\nğŸ“¦ MS MARCO ë¡œë”©...")
            dataset = load_dataset("ms_marco", "v2.1", split=f"train[:{self.size}]")
            print(f"   âœ… ë¡œë“œ ì™„ë£Œ: {len(dataset)} í•­ëª©")
            
            documents = []
            queries = []
            doc_id_map = {}
            
            for idx, example in enumerate(dataset):
                if idx % 1000 == 0:
                    print(f"   ì§„í–‰: {idx}/{len(dataset)}")
                
                query = example.get("query", "")
                passages = example.get("passages", {})
                answers = example.get("answers", [])
                
                if not query:
                    continue
                
                relevant_doc_ids = []
                
                # Passageë¥¼ ë¬¸ì„œë¡œ ë³€í™˜
                for p_idx, passage in enumerate(passages.get("passage_text", [])):
                    doc_id = f"msmarco_{idx}_p{p_idx}"
                    
                    documents.append({
                        "id": doc_id,
                        "content": passage,
                        "title": f"Passage {idx}-{p_idx}",
                        "metadata": {
                            "source": "msmarco",
                            "query_idx": idx,
                            "passage_idx": p_idx
                        }
                    })
                    
                    # ê´€ë ¨ ë¬¸ì„œë¡œ í‘œì‹œ
                    if passages.get("is_selected", [])[p_idx]:
                        relevant_doc_ids.append(doc_id)
                
                # ì¿¼ë¦¬ ìƒì„±
                queries.append({
                    "query": query,
                    "relevant_doc_ids": relevant_doc_ids,
                    "expected_answer": answers[0] if answers else "",
                    "difficulty": "medium",
                    "query_type": "qa",
                    "metadata": {
                        "source": "msmarco",
                        "query_idx": idx
                    }
                })
            
            # ìµœì¢… ë°ì´í„°ì…‹
            evaluation_dataset = {
                "name": f"MS-MARCO-{self.size}",
                "description": f"MS MARCO passages for retrieval evaluation ({self.size} queries)",
                "documents": documents,
                "queries": queries,
                "metadata": {
                    "source": "ms_marco/v2.1",
                    "total_documents": len(documents),
                    "total_queries": len(queries)
                }
            }
            
            # ì €ì¥
            output_file = self.output_dir / f"msmarco_{self.size}_eval.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(evaluation_dataset, f, indent=2, ensure_ascii=False)
            
            file_size = output_file.stat().st_size / 1024 / 1024
            print(f"\nâœ… ì™„ë£Œ: {output_file}")
            print(f"   ğŸ“Š í†µê³„:")
            print(f"      - ë¬¸ì„œ: {len(documents)}")
            print(f"      - ì¿¼ë¦¬: {len(queries)}")
            print(f"      - íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")
            
            # ìë™ DB ë“±ë¡
            if self.auto_register:
                register_dataset_to_db(str(output_file))
            
            return str(output_file)
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None


def download_all_beir(output_dir: str = "backend/datasets", auto_register: bool = True) -> List[str]:
    """ëª¨ë“  BEIR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"""
    
    print("\nğŸ¦› ëª¨ë“  BEIR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    print(f"   ë°ì´í„°ì…‹ ìˆ˜: {len(BEIR_DATASETS)}")
    
    downloaded = []
    
    for dataset_name in BEIR_DATASETS.keys():
        print(f"\n{'='*60}")
        preparer = BEIRDatasetPreparer(dataset_name, output_dir, auto_register=auto_register)
        output_file = preparer.prepare()
        
        if output_file:
            downloaded.append(output_file)
    
    print(f"\n{'='*60}")
    print(f"âœ… ì „ì²´ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(downloaded)}/{len(BEIR_DATASETS)}")
    
    return downloaded


def verify_dataset(dataset_path: str):
    """ë°ì´í„°ì…‹ ê²€ì¦ ë° í†µê³„ ì¶œë ¥"""
    
    print(f"\nğŸ” ë°ì´í„°ì…‹ ê²€ì¦: {dataset_path}")
    
    try:
        with open(dataset_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        print(f"\nğŸ“Š ê¸°ë³¸ ì •ë³´:")
        print(f"   ì´ë¦„: {data.get('name', 'Unknown')}")
        print(f"   ì„¤ëª…: {data.get('description', 'No description')}")
        
        documents = data.get("documents", [])
        queries = data.get("queries", [])
        
        print(f"\nğŸ“Š í†µê³„:")
        print(f"   ë¬¸ì„œ ìˆ˜: {len(documents)}")
        print(f"   ì¿¼ë¦¬ ìˆ˜: {len(queries)}")
        
        if documents:
            doc_lengths = [len(d["content"]) for d in documents]
            print(f"\nğŸ“„ ë¬¸ì„œ ê¸¸ì´:")
            print(f"   í‰ê· : {sum(doc_lengths) / len(doc_lengths):.0f} ì")
            print(f"   ìµœì†Œ: {min(doc_lengths)} ì")
            print(f"   ìµœëŒ€: {max(doc_lengths)} ì")
        
        if queries:
            query_lengths = [len(q["query"]) for q in queries]
            print(f"\nâ“ ì¿¼ë¦¬ ê¸¸ì´:")
            print(f"   í‰ê· : {sum(query_lengths) / len(query_lengths):.0f} ì")
            print(f"   ìµœì†Œ: {min(query_lengths)} ì")
            print(f"   ìµœëŒ€: {max(query_lengths)} ì")
            
            # ì¿¼ë¦¬ íƒ€ì… ë¶„í¬
            query_types = {}
            for q in queries:
                qt = q.get("query_type", "unknown")
                query_types[qt] = query_types.get(qt, 0) + 1
            
            print(f"\nğŸ·ï¸  ì¿¼ë¦¬ íƒ€ì… ë¶„í¬:")
            for qt, count in query_types.items():
                print(f"   {qt}: {count}")
        
        print(f"\nâœ… ê²€ì¦ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {e}")


def list_datasets():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡"""
    
    print("\nğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:\n")
    
    print("BEIR ë²¤ì¹˜ë§ˆí¬:")
    for name, info in BEIR_DATASETS.items():
        print(f"  - {name:15s} : {info['description']} ({info['size']})")
    
    print("\nê¸°íƒ€:")
    print(f"  - wikipedia     : Wikipedia articles with auto-generated queries")
    print(f"  - msmarco       : MS MARCO passages for retrieval")


def main():
    parser = argparse.ArgumentParser(
        description="ë²”ìš© ë°ì´í„°ì…‹ ì¤€ë¹„ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    subparsers = parser.add_subparsers(dest="command", help="ëª…ë ¹ì–´")
    
    # BEIR ëª…ë ¹
    beir_parser = subparsers.add_parser("beir", help="BEIR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
    beir_parser.add_argument("--name", required=True, choices=BEIR_DATASETS.keys(), help="BEIR ë°ì´í„°ì…‹ ì´ë¦„")
    beir_parser.add_argument("--sample", type=int, help="ìƒ˜í”Œ í¬ê¸°")
    beir_parser.add_argument("--output-dir", default="backend/datasets", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    beir_parser.add_argument("--no-register", action="store_true", help="ìë™ DB ë“±ë¡ ê±´ë„ˆë›°ê¸°")
    
    # ëª¨ë“  BEIR ë‹¤ìš´ë¡œë“œ
    download_all_parser = subparsers.add_parser("download-all", help="ëª¨ë“  BEIR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
    download_all_parser.add_argument("--output-dir", default="backend/datasets", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    download_all_parser.add_argument("--no-register", action="store_true", help="ìë™ DB ë“±ë¡ ê±´ë„ˆë›°ê¸°")
    
    # Wikipedia ëª…ë ¹
    wiki_parser = subparsers.add_parser("wikipedia", help="Wikipedia ë°ì´í„°ì…‹ ìƒì„±")
    wiki_parser.add_argument("--size", type=int, default=1000, help="ë¬¸ì„œ ìˆ˜")
    wiki_parser.add_argument("--queries-per-doc", type=int, default=2, help="ë¬¸ì„œë‹¹ ì¿¼ë¦¬ ìˆ˜")
    wiki_parser.add_argument("--output-dir", default="backend/datasets", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    wiki_parser.add_argument("--no-register", action="store_true", help="ìë™ DB ë“±ë¡ ê±´ë„ˆë›°ê¸°")
    
    # MS MARCO ëª…ë ¹
    msmarco_parser = subparsers.add_parser("msmarco", help="MS MARCO ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
    msmarco_parser.add_argument("--size", type=int, default=10000, help="Passage ìˆ˜")
    msmarco_parser.add_argument("--output-dir", default="backend/datasets", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    msmarco_parser.add_argument("--no-register", action="store_true", help="ìë™ DB ë“±ë¡ ê±´ë„ˆë›°ê¸°")
    
    # ê²€ì¦ ëª…ë ¹
    verify_parser = subparsers.add_parser("verify", help="ë°ì´í„°ì…‹ ê²€ì¦")
    verify_parser.add_argument("--dataset", required=True, help="ê²€ì¦í•  ë°ì´í„°ì…‹ ê²½ë¡œ")
    
    # ëª©ë¡ ëª…ë ¹
    list_parser = subparsers.add_parser("list", help="ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # ëª…ë ¹ ì‹¤í–‰
    if args.command == "beir":
        auto_register = not args.no_register
        preparer = BEIRDatasetPreparer(args.name, args.output_dir, args.sample, auto_register)
        preparer.prepare()
    
    elif args.command == "download-all":
        auto_register = not args.no_register
        download_all_beir(args.output_dir, auto_register)
    
    elif args.command == "wikipedia":
        auto_register = not args.no_register
        preparer = WikipediaDatasetPreparer(args.size, args.queries_per_doc, args.output_dir, auto_register)
        preparer.prepare()
    
    elif args.command == "msmarco":
        auto_register = not args.no_register
        preparer = MSMARCODatasetPreparer(args.size, args.output_dir, auto_register)
        preparer.prepare()
    
    elif args.command == "verify":
        verify_dataset(args.dataset)
    
    elif args.command == "list":
        list_datasets()


if __name__ == "__main__":
    main()

