#!/usr/bin/env python3
"""
FRAMES ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

Googleì˜ FRAMES (Factuality, Retrieval, And reasoning MEasurement Set) ë²¤ì¹˜ë§ˆí¬ë¥¼
ë‹¤ìš´ë¡œë“œí•˜ê³  RAG í‰ê°€ ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

Usage:
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (Wikipedia ë‚´ìš© ì—†ì´)
    python backend/scripts/download_frames.py --sample --no-fetch-wikipedia
    
    # ì™„ì „í•œ ë°ì´í„°ì…‹ (Wikipedia ë‚´ìš© í¬í•¨) - ì¶”ì²œ
    python backend/scripts/download_frames.py --fetch-wikipedia
    
    # 200ê°œ ì¿¼ë¦¬ë§Œ + Wikipedia ë‚´ìš©
    python backend/scripts/download_frames.py --max-queries 200 --fetch-wikipedia
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Any, Optional

# datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹œë„
try:
    from datasets import load_dataset
except ImportError:
    print("âŒ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install datasets")
    sys.exit(1)

# Wikipedia APIìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import requests
except ImportError:
    print("âŒ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install requests")
    sys.exit(1)


class FramesDownloader:
    """FRAMES ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë”"""
    
    def __init__(
        self,
        output_path: str = "backend/datasets/frames_eval.json",
        fetch_wikipedia: bool = False,
        max_queries: Optional[int] = None,
        sample: bool = False
    ):
        self.output_path = Path(output_path)
        self.fetch_wikipedia = fetch_wikipedia
        self.max_queries = max_queries if not sample else 100
        self.sample = sample
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_path.parent.mkdir(parents=True, exist_ok=True)
    
    def fetch_wikipedia_content(self, title: str) -> Optional[str]:
        """Wikipedia APIë¥¼ í†µí•´ ì‹¤ì œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        url = "https://en.wikipedia.org/api/rest_v1/page/summary/" + title.replace(" ", "_")
        
        # Wikipedia requires a User-Agent header
        headers = {
            'User-Agent': 'RAG-Evaluation-System/1.0 (Educational/Research Purpose; Contact: admin@example.com)'
        }
        
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            # extract í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš© (ìš”ì•½)
            content = data.get("extract", "")
            
            if not content:
                return None
            
            return content
            
        except Exception as e:
            return None
    
    def convert_to_evaluation_format(self, dataset) -> Dict[str, Any]:
        """FRAMES ë°ì´í„°ì…‹ì„ í‰ê°€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        
        print("\nğŸ“ ë°ì´í„°ì…‹ ë³€í™˜ ì¤‘...", flush=True)
        
        documents = []
        queries = []
        doc_id_map = {}  # Wikipedia ì œëª© â†’ ë¬¸ì„œ ID ë§¤í•‘
        
        # ì¿¼ë¦¬ ì œí•œ ì ìš©
        total_examples = len(dataset)
        if self.max_queries:
            dataset = dataset.select(range(min(self.max_queries, total_examples)))
        
        print(f"   ì²˜ë¦¬í•  ì¿¼ë¦¬ ìˆ˜: {len(dataset)}", flush=True)
        
        for idx, example in enumerate(dataset):
            if idx % 50 == 0:
                print(f"   ì§„í–‰: {idx}/{len(dataset)} ì¿¼ë¦¬", flush=True)
            
            # ì¿¼ë¦¬ ì •ë³´
            question = example.get("Prompt", "")
            answer = example.get("Answer", "")
            wikipedia_links = example.get("wiki_links", [])
            reasoning_type = example.get("reasoning_types", ["unknown"])[0]
            
            # Parse wiki_links if it's a string (JSON format)
            if isinstance(wikipedia_links, str):
                import ast
                try:
                    wikipedia_links = ast.literal_eval(wikipedia_links)
                except:
                    wikipedia_links = []
            
            if not question:
                continue
            
            # ê´€ë ¨ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
            relevant_doc_ids = []
            
            # Wikipedia ë§í¬ì—ì„œ ë¬¸ì„œ ìƒì„±
            for wiki_link in wikipedia_links:
                # ì œëª© ì¶”ì¶œ
                title = wiki_link.split("/")[-1].replace("_", " ")
                
                # ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œì¸ì§€ í™•ì¸
                if title in doc_id_map:
                    relevant_doc_ids.append(doc_id_map[title])
                    continue
                
                # ìƒˆ ë¬¸ì„œ ID ìƒì„±
                doc_id = f"frames_q{idx}_doc{len(documents)}"
                doc_id_map[title] = doc_id
                relevant_doc_ids.append(doc_id)
                
                # Wikipedia ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                if self.fetch_wikipedia:
                    if len(documents) % 100 == 0 and len(documents) > 0:
                        print(f"   ğŸ“¥ Wikipedia ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘: {len(documents)}/{len(doc_id_map)} ë¬¸ì„œ", flush=True)
                    content = self.fetch_wikipedia_content(title)
                    if content is None:
                        content = f"[Wikipedia content for '{title}']"
                    # Rate limiting (be respectful to Wikipedia API)
                    time.sleep(0.2)
                else:
                    content = f"[Placeholder content for Wikipedia article: {title}]"
                
                # ë¬¸ì„œ ìƒì„±
                document = {
                    "id": doc_id,
                    "content": content,
                    "title": title,
                    "metadata": {
                        "source": "frames",
                        "wikipedia_url": wiki_link,
                        "question_idx": idx,
                        "content_length": len(content)
                    }
                }
                documents.append(document)
            
            # ì¿¼ë¦¬ ìƒì„±
            query = {
                "query": question,
                "relevant_doc_ids": relevant_doc_ids,
                "expected_answer": answer,
                "difficulty": "hard",  # FRAMESëŠ” ëŒ€ë¶€ë¶„ ì–´ë ¤ì›€
                "query_type": "multi-hop" if len(relevant_doc_ids) > 1 else "single-hop",
                "metadata": {
                    "source": "frames",
                    "question_idx": idx,
                    "reasoning_type": reasoning_type,
                    "num_wikipedia_links": len(wikipedia_links)
                }
            }
            queries.append(query)
        
        # ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì¡°
        evaluation_dataset = {
            "name": "FRAMES-RAG",
            "description": "Google FRAMES benchmark for RAG evaluation with multi-hop reasoning",
            "documents": documents,
            "queries": queries,
            "metadata": {
                "source": "google/frames-benchmark",
                "version": "2024",
                "total_examples": total_examples,
                "converted_queries": len(queries),
                "converted_documents": len(documents),
                "fetched_wikipedia": self.fetch_wikipedia
            }
        }
        
        return evaluation_dataset
    
    def download(self):
        """ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜"""
        
        print("ğŸ¦› FRAMES ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘", flush=True)
        print(f"   ì¶œë ¥ ê²½ë¡œ: {self.output_path}", flush=True)
        print(f"   Wikipedia ê°€ì ¸ì˜¤ê¸°: {'âœ…' if self.fetch_wikipedia else 'âŒ'}", flush=True)
        print(f"   ìµœëŒ€ ì¿¼ë¦¬ ìˆ˜: {self.max_queries or 'ì „ì²´ (824)'}", flush=True)
        
        # 1. HuggingFaceì—ì„œ FRAMES ë°ì´í„°ì…‹ ë¡œë“œ
        print("\nğŸ“¦ HuggingFaceì—ì„œ ë°ì´í„°ì…‹ ë¡œë”©...", flush=True)
        try:
            dataset = load_dataset("google/frames-benchmark", split="test")
            print(f"   âœ… ë¡œë“œ ì™„ë£Œ: {len(dataset)} ì˜ˆì œ", flush=True)
        except Exception as e:
            print(f"   âŒ ë¡œë“œ ì‹¤íŒ¨: {e}", flush=True)
            return False
        
        # 2. í‰ê°€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        evaluation_dataset = self.convert_to_evaluation_format(dataset)
        
        # 3. JSON íŒŒì¼ë¡œ ì €ì¥
        print(f"\nğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘: {self.output_path}", flush=True)
        with open(self.output_path, "w", encoding="utf-8") as f:
            json.dump(evaluation_dataset, f, indent=2, ensure_ascii=False)
        
        # 4. í†µê³„ ì¶œë ¥
        file_size = self.output_path.stat().st_size / 1024 / 1024  # MB
        print(f"\nâœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!", flush=True)
        print(f"   ì¿¼ë¦¬: {len(evaluation_dataset['queries'])}, ë¬¸ì„œ: {len(evaluation_dataset['documents'])}, í¬ê¸°: {file_size:.2f}MB", flush=True)
        
        return True


def main():
    parser = argparse.ArgumentParser(
        description="FRAMES ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (100 queries, placeholder content)
  python backend/scripts/download_frames.py --sample --no-fetch-wikipedia
  
  # ì™„ì „í•œ ë°ì´í„°ì…‹ (824 queries, real Wikipedia content) - ì¶”ì²œ
  python backend/scripts/download_frames.py --fetch-wikipedia
  
  # 200 queries + real Wikipedia content
  python backend/scripts/download_frames.py --max-queries 200 --fetch-wikipedia
  
  # ì»¤ìŠ¤í…€ ì¶œë ¥ ê²½ë¡œ
  python backend/scripts/download_frames.py --fetch-wikipedia --output custom/path.json
        """
    )
    
    parser.add_argument(
        "--output",
        type=str,
        default="backend/datasets/frames_eval.json",
        help="ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ (default: backend/datasets/frames_eval.json)"
    )
    
    parser.add_argument(
        "--fetch-wikipedia",
        action="store_true",
        help="ì‹¤ì œ Wikipedia ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ì‹œê°„ ì†Œìš”, í•˜ì§€ë§Œ ê¶Œì¥)"
    )
    
    parser.add_argument(
        "--no-fetch-wikipedia",
        dest="fetch_wikipedia",
        action="store_false",
        help="Placeholder ë‚´ìš©ë§Œ ì‚¬ìš© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)"
    )
    
    parser.add_argument(
        "--max-queries",
        type=int,
        help="ìµœëŒ€ ì¿¼ë¦¬ ìˆ˜ ì œí•œ (ì˜ˆ: 200)"
    )
    
    parser.add_argument(
        "--sample",
        action="store_true",
        help="ìƒ˜í”Œ ëª¨ë“œ (100 queriesë§Œ)"
    )
    
    parser.add_argument(
        "--register",
        action="store_true",
        default=True,
        help="ë‹¤ìš´ë¡œë“œ í›„ ìë™ìœ¼ë¡œ DBì— ë“±ë¡ (ê¸°ë³¸: True)"
    )
    
    parser.add_argument(
        "--no-register",
        dest="register",
        action="store_false",
        help="ìë™ DB ë“±ë¡ ê±´ë„ˆë›°ê¸°"
    )
    
    parser.set_defaults(fetch_wikipedia=False)
    
    args = parser.parse_args()
    
    # ë‹¤ìš´ë¡œë” ì‹¤í–‰
    downloader = FramesDownloader(
        output_path=args.output,
        fetch_wikipedia=args.fetch_wikipedia,
        max_queries=args.max_queries,
        sample=args.sample
    )
    
    success = downloader.download()
    
    if success:
        # ìë™ DB ë“±ë¡
        if args.register:
            print(f"\nğŸ“ ë°ì´í„°ë² ì´ìŠ¤ì— ìë™ ë“±ë¡ ì¤‘...")
            try:
                import subprocess
                import os
                result = subprocess.run(
                    [
                        sys.executable,
                        str(Path(__file__).parent / "dataset_registry.py"),
                        "register",
                        args.output
                    ],
                    env={**os.environ, "DATABASE_URL": os.environ.get("DATABASE_URL", "postgresql://postgres:postgres@localhost:5433/rag_evaluation")},
                    capture_output=True,
                    text=True
                )
                
                if result.returncode == 0:
                    print(f"   âœ… ë°ì´í„°ë² ì´ìŠ¤ ë“±ë¡ ì™„ë£Œ!")
                else:
                    print(f"   âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ë“±ë¡ ì‹¤íŒ¨")
            except Exception as e:
                print(f"   âš ï¸  ìë™ ë“±ë¡ ì‹¤íŒ¨: {e}")
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == "__main__":
    main()

