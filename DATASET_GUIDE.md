# Dataset Preparation Guide

í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ íŒ¨í‚¤ì§€
pip install datasets requests

# BEIR ë²¤ì¹˜ë§ˆí¬ (ì„ íƒ)
pip install beir
```

## ğŸ¯ ì§€ì›í•˜ëŠ” ë°ì´í„°ì…‹

### 1. FRAMES (Google)
- **ì„¤ëª…**: Multi-hop reasoningì„ ìœ„í•œ RAG ë²¤ì¹˜ë§ˆí¬
- **í¬ê¸°**: 824 question-answer pairs
- **íŠ¹ì§•**: Wikipedia ê¸°ë°˜, ë³µì¡í•œ ì¶”ë¡  í•„ìš”
- **ì¶”ì²œ**: RAG í‰ê°€ì— ê°€ì¥ ì í•©

### 2. BEIR Benchmark
- **SciFact**: ê³¼í•™ ë…¼ë¬¸ ê²€ì¦ (5K docs)
- **NFCorpus**: ì˜ì–‘í•™ (3.6K docs)
- **HotpotQA**: Multi-hop QA (5M docs)
- **FiQA**: ê¸ˆìœµ QA (57K docs)
- **TREC-COVID**: COVID-19 ì—°êµ¬ (171K docs)

### 3. Wikipedia
- **ì„¤ëª…**: Wikipedia ë¬¸ì„œ + ìë™ ìƒì„± ì¿¼ë¦¬
- **í¬ê¸°**: ì‚¬ìš©ì ì •ì˜ (1K~100K)
- **íŠ¹ì§•**: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©

### 4. MS MARCO
- **ì„¤ëª…**: Microsoftì˜ QA ë°ì´í„°ì…‹
- **í¬ê¸°**: ì‚¬ìš©ì ì •ì˜ (10K~1M)
- **íŠ¹ì§•**: ì‹¤ì œ ê²€ìƒ‰ ì¿¼ë¦¬ ê¸°ë°˜

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### FRAMES ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

#### ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (Wikipedia ë‚´ìš© ì—†ì´)
```bash
cd /Users/johongcheol/rag-evaluation-web-ui

# 100ê°œ ìƒ˜í”Œ, placeholder ë‚´ìš© (ìë™ DB ë“±ë¡)
python backend/scripts/download_frames.py --sample --no-fetch-wikipedia
```

#### ì™„ì „í•œ ë°ì´í„°ì…‹ (Wikipedia ë‚´ìš© í¬í•¨) â­ ì¶”ì²œ
```bash
# ì „ì²´ ë°ì´í„°ì…‹ (824 queries) + Wikipedia ë‚´ìš© (ìë™ DB ë“±ë¡)
python backend/scripts/download_frames.py --fetch-wikipedia

# 200ê°œ ì¿¼ë¦¬ + Wikipedia ë‚´ìš© (ìë™ DB ë“±ë¡)
python backend/scripts/download_frames.py --max-queries 200 --fetch-wikipedia

# ì»¤ìŠ¤í…€ ì¶œë ¥ ê²½ë¡œ
python backend/scripts/download_frames.py \
    --fetch-wikipedia \
    --max-queries 100 \
    --output backend/datasets/frames_100.json

# DB ë“±ë¡ ê±´ë„ˆë›°ê¸°
python backend/scripts/download_frames.py --fetch-wikipedia --no-register
```

**âš ï¸ ì£¼ì˜**: `--fetch-wikipedia` ì˜µì…˜ì€ ì‹¤ì œ Wikipedia APIë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ:
- ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤ (100 queries â‰ˆ 2-3ë¶„)
- Rate limitingì´ ì ìš©ë©ë‹ˆë‹¤ (0.1ì´ˆ/ìš”ì²­)
- í•˜ì§€ë§Œ **ì‹¤ì œ RAG í‰ê°€ë¥¼ ìœ„í•´ì„œëŠ” í•„ìˆ˜**ì…ë‹ˆë‹¤!

**âœ¨ ìë™ DB ë“±ë¡**: ë‹¤ìš´ë¡œë“œí•œ ë°ì´í„°ì…‹ì€ ìë™ìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ì— ë“±ë¡ë©ë‹ˆë‹¤!

---

### BEIR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

#### ê°œë³„ ë‹¤ìš´ë¡œë“œ
```bash
cd /Users/johongcheol/rag-evaluation-web-ui

# SciFact (ì‘ì€ ë°ì´í„°ì…‹, í…ŒìŠ¤íŠ¸ìš©) - ìë™ DB ë“±ë¡
python backend/scripts/prepare_dataset.py beir --name scifact

# HotpotQA (ìƒ˜í”Œë§ í•„ìš”, ë„ˆë¬´ í¼) - ìë™ DB ë“±ë¡
python backend/scripts/prepare_dataset.py beir --name hotpotqa --sample 1000

# FiQA (ê¸ˆìœµ QA) - ìë™ DB ë“±ë¡
python backend/scripts/prepare_dataset.py beir --name fiqa

# DB ë“±ë¡ ê±´ë„ˆë›°ê¸°
python backend/scripts/prepare_dataset.py beir --name scifact --no-register
```

#### ëª¨ë“  BEIR ë°ì´í„°ì…‹ í•œë²ˆì— ë‹¤ìš´ë¡œë“œ â­ ìƒˆë¡œìš´ ê¸°ëŠ¥!
```bash
# ëª¨ë“  BEIR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ìë™ DB ë“±ë¡
python backend/scripts/prepare_dataset.py download-all

# DB ë“±ë¡ ê±´ë„ˆë›°ê¸°
python backend/scripts/prepare_dataset.py download-all --no-register
```

**ë‹¤ìš´ë¡œë“œë˜ëŠ” ë°ì´í„°ì…‹:**
- scifact (5K docs)
- nfcorpus (3.6K docs)
- hotpotqa (5M docs)
- fiqa (57K docs)
- trec-covid (171K docs)

#### ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡
```bash
python backend/scripts/prepare_dataset.py list
```

---

### Wikipedia ë°ì´í„°ì…‹ ìƒì„±

```bash
# 1000ê°œ ë¬¸ì„œ + 2ê°œ ì¿¼ë¦¬/ë¬¸ì„œ (ìë™ DB ë“±ë¡)
python backend/scripts/prepare_dataset.py wikipedia --size 1000

# 10000ê°œ ë¬¸ì„œ + 3ê°œ ì¿¼ë¦¬/ë¬¸ì„œ (ìë™ DB ë“±ë¡)
python backend/scripts/prepare_dataset.py wikipedia --size 10000 --queries-per-doc 3

# ì»¤ìŠ¤í…€ ì¶œë ¥ ê²½ë¡œ
python backend/scripts/prepare_dataset.py wikipedia \
    --size 5000 \
    --queries-per-doc 2 \
    --output backend/datasets/wiki_5k.json

# DB ë“±ë¡ ê±´ë„ˆë›°ê¸°
python backend/scripts/prepare_dataset.py wikipedia --size 1000 --no-register
```

---

### MS MARCO ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

```bash
# 10K passages (ê¸°ë³¸) - ìë™ DB ë“±ë¡
python backend/scripts/prepare_dataset.py msmarco --size 10000

# 50K passages - ìë™ DB ë“±ë¡
python backend/scripts/prepare_dataset.py msmarco --size 50000

# DB ë“±ë¡ ê±´ë„ˆë›°ê¸°
python backend/scripts/prepare_dataset.py msmarco --size 10000 --no-register
```

---

### ë°ì´í„°ì…‹ ê²€ì¦

```bash
# ë‹¤ìš´ë¡œë“œí•œ ë°ì´í„°ì…‹ ê²€ì¦
python backend/scripts/prepare_dataset.py verify --dataset backend/datasets/frames_eval.json

# í†µê³„ ì¶œë ¥:
# - ë¬¸ì„œ/ì¿¼ë¦¬ ê°œìˆ˜
# - ë¬¸ì„œ ê¸¸ì´ ë¶„í¬
# - ì¿¼ë¦¬ ë‚œì´ë„ ë¶„í¬
# - ì¿¼ë¦¬ íƒ€ì… ë¶„í¬
# - ê´€ë ¨ ë¬¸ì„œ ìˆ˜
```

---

### ë°ì´í„°ì…‹ ê´€ë¦¬ (DB)

#### ë“±ë¡ëœ ë°ì´í„°ì…‹ ëª©ë¡ í™•ì¸
```bash
# ë°ì´í„°ë² ì´ìŠ¤ì— ë“±ë¡ëœ ëª¨ë“  ë°ì´í„°ì…‹ í™•ì¸
python backend/scripts/dataset_registry.py list
```

#### ìˆ˜ë™ìœ¼ë¡œ ë°ì´í„°ì…‹ ë“±ë¡
```bash
# íŠ¹ì • ë°ì´í„°ì…‹ íŒŒì¼ì„ ìˆ˜ë™ìœ¼ë¡œ ë“±ë¡
python backend/scripts/dataset_registry.py register backend/datasets/frames_eval.json
```

#### ëª¨ë“  ë°ì´í„°ì…‹ ìë™ ë“±ë¡
```bash
# datasets ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  JSON íŒŒì¼ì„ ìë™ ë“±ë¡
python backend/scripts/dataset_registry.py auto-register

# ë‹¤ë¥¸ ë””ë ‰í† ë¦¬ ì§€ì •
python backend/scripts/dataset_registry.py auto-register --dir /path/to/datasets
```

---

## ğŸ“Š ë°ì´í„°ì…‹ í˜•ì‹

ìƒì„±ëœ JSON íŒŒì¼ì€ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¦…ë‹ˆë‹¤:

```json
{
  "name": "FRAMES-RAG",
  "description": "Google FRAMES benchmark for RAG evaluation",
  "documents": [
    {
      "id": "frames_q0_doc0",
      "content": "Wikipedia article content...",
      "title": "Article Title",
      "metadata": {
        "source": "frames",
        "wikipedia_url": "https://en.wikipedia.org/wiki/...",
        "question_idx": 0,
        "content_length": 1234
      }
    }
  ],
  "queries": [
    {
      "query": "What is the capital of France?",
      "relevant_doc_ids": ["frames_q0_doc0", "frames_q0_doc1"],
      "expected_answer": "Paris",
      "difficulty": "hard",
      "query_type": "multi-hop",
      "metadata": {
        "source": "frames",
        "question_idx": 0,
        "reasoning_type": "multi-hop",
        "num_wikipedia_links": 2
      }
    }
  ],
  "metadata": {
    "source": "google/frames-benchmark",
    "version": "2024",
    "total_examples": 824,
    "converted_queries": 820,
    "converted_documents": 3456,
    "fetched_wikipedia": true
  }
}
```

---

## ğŸ’¡ ì¶”ì²œ ì›Œí¬í”Œë¡œìš°

### 1. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ê°œë°œ ì¤‘)
```bash
# ì‘ì€ ìƒ˜í”Œë¡œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸ (ìë™ DB ë“±ë¡)
python backend/scripts/download_frames.py --sample --no-fetch-wikipedia
# â†’ backend/datasets/frames_eval.json (100 queries, placeholder content)
# â†’ ìë™ìœ¼ë¡œ DBì— ë“±ë¡ë¨!
```

### 2. ì‹¤ì œ í‰ê°€ (í”„ë¡œë•ì…˜)
```bash
# Wikipedia ë‚´ìš©ì„ í¬í•¨í•œ ì™„ì „í•œ ë°ì´í„°ì…‹ (ìë™ DB ë“±ë¡)
python backend/scripts/download_frames.py --fetch-wikipedia
# â†’ backend/datasets/frames_eval.json (824 queries, real Wikipedia content)
# â†’ ìë™ìœ¼ë¡œ DBì— ë“±ë¡ë¨!
```

### 3. ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ë²¤ì¹˜ë§ˆí‚¹ (ëª¨ë‘ ìë™ DB ë“±ë¡)
```bash
# FRAMES (multi-hop reasoning)
python backend/scripts/download_frames.py --fetch-wikipedia --max-queries 200

# ëª¨ë“  BEIR ë°ì´í„°ì…‹ í•œë²ˆì— ë‹¤ìš´ë¡œë“œ
python backend/scripts/prepare_dataset.py download-all

# ë˜ëŠ” ê°œë³„ ë‹¤ìš´ë¡œë“œ
python backend/scripts/prepare_dataset.py beir --name scifact
python backend/scripts/prepare_dataset.py beir --name fiqa

# Wikipedia (general knowledge)
python backend/scripts/prepare_dataset.py wikipedia --size 1000

# ë“±ë¡ëœ ë°ì´í„°ì…‹ í™•ì¸
python backend/scripts/dataset_registry.py list
```

### 4. ê¸°ì¡´ ë°ì´í„°ì…‹ ì¼ê´„ ë“±ë¡
```bash
# datasets ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  JSON íŒŒì¼ì„ DBì— ë“±ë¡
python backend/scripts/dataset_registry.py auto-register
```

---

## ğŸ”§ APIë¥¼ í†µí•œ ë°ì´í„°ì…‹ ì‚¬ìš©

ë°ì´í„°ì…‹ì´ ìë™ìœ¼ë¡œ DBì— ë“±ë¡ë˜ë¯€ë¡œ, ë°”ë¡œ APIë¥¼ í†µí•´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# 1. ë“±ë¡ëœ ë°ì´í„°ì…‹ ëª©ë¡ í™•ì¸
curl http://localhost:8001/api/datasets

# 2. RAG ì„¤ì • ìƒì„±
curl -X POST http://localhost:8001/api/rags \
  -H "Content-Type: application/json" \
  -d '{
    "name": "My RAG",
    "chunking": {"module": "recursive", "params": {"chunk_size": 512}},
    "embedding": {"module": "bge_m3", "params": {}},
    "reranking": {"module": "cross_encoder", "params": {}}
  }'

# 3. í‰ê°€ ì‹¤í–‰ (dataset_idëŠ” ìœ„ì—ì„œ í™•ì¸í•œ ID ì‚¬ìš©)
curl -X POST http://localhost:8001/api/evaluations \
  -H "Content-Type: application/json" \
  -d '{
    "rag_id": 1,
    "dataset_id": 1,
    "metrics": ["precision", "recall", "mrr", "ndcg"]
  }'

# 4. í‰ê°€ ê²°ê³¼ í™•ì¸
curl http://localhost:8001/api/evaluations/1
```

---

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
backend/
â”œâ”€â”€ datasets/                    # ë‹¤ìš´ë¡œë“œëœ ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ frames_eval.json        # FRAMES ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ beir_scifact_eval.json  # BEIR SciFact
â”‚   â”œâ”€â”€ wikipedia_1000_eval.json # Wikipedia
â”‚   â””â”€â”€ .beir/                  # BEIR ìºì‹œ
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_frames.py      # FRAMES ë‹¤ìš´ë¡œë”
â”‚   â””â”€â”€ prepare_dataset.py      # ë²”ìš© ë°ì´í„°ì…‹ ì¤€ë¹„ ë„êµ¬
```

---

## âš¡ ì„±ëŠ¥ íŒ

### FRAMES ë‹¤ìš´ë¡œë“œ ìµœì í™”
- **ë¹ ë¥¸ í…ŒìŠ¤íŠ¸**: `--no-fetch-wikipedia` (ì´ˆ ë‹¨ìœ„)
- **ì‹¤ì œ í‰ê°€**: `--fetch-wikipedia` (ë¶„ ë‹¨ìœ„, í•˜ì§€ë§Œ í•„ìˆ˜!)
- **ìƒ˜í”Œë§**: `--sample` ë˜ëŠ” `--max-queries 100` (ê°œë°œ ì¤‘)

### BEIR ë‹¤ìš´ë¡œë“œ ìµœì í™”
- **ì‘ì€ ë°ì´í„°ì…‹**: scifact, nfcorpus (ë¹ ë¦„)
- **í° ë°ì´í„°ì…‹**: hotpotqa, trec-covid (ìƒ˜í”Œë§ í•„ìˆ˜)
- **ìƒ˜í”Œë§**: `--sample 1000` (ì²˜ìŒ 1000ê°œë§Œ)

### Wikipedia ë‹¤ìš´ë¡œë“œ ìµœì í™”
- **í…ŒìŠ¤íŠ¸**: `--size 1000` (1-2ë¶„)
- **ì¤‘ê°„**: `--size 10000` (10-15ë¶„)
- **ëŒ€ê·œëª¨**: `--size 100000` (1-2ì‹œê°„)

---

## ğŸ› ë¬¸ì œ í•´ê²°

### "datasets library not installed"
```bash
pip install datasets
```

### "BEIR not installed"
```bash
pip install beir
```

### "Failed to fetch Wikipedia"
- Rate limiting: ì ì‹œ ê¸°ë‹¤ë ¸ë‹¤ê°€ ì¬ì‹œë„
- ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ: ì¸í„°ë„· ì—°ê²° í™•ì¸
- API ì˜¤ë¥˜: `--no-fetch-wikipedia` ì‚¬ìš© (placeholder)

### "Dataset verification failed"
```bash
# ë°ì´í„°ì…‹ ê²€ì¦ ì‹¤í–‰
python backend/scripts/prepare_dataset.py verify --dataset backend/datasets/frames_eval.json

# ë¬¸ì œê°€ ìˆìœ¼ë©´ ì¬ë‹¤ìš´ë¡œë“œ
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **FRAMES**: https://arxiv.org/abs/2409.12941
- **BEIR**: https://github.com/beir-cellar/beir
- **MS MARCO**: https://microsoft.github.io/msmarco/
- **Wikipedia Dataset**: https://huggingface.co/datasets/wikipedia

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. âœ… ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (ìë™ DB ë“±ë¡)
2. âœ… ë°ì´í„°ì…‹ ê²€ì¦
3. âœ… ë°ì´í„°ë² ì´ìŠ¤ì— ìë™ ë“±ë¡ë¨!
4. â†’ RAG ì„¤ì • ìƒì„± (API)
5. â†’ í‰ê°€ ì‹¤í–‰ (API)
6. â†’ ê²°ê³¼ ë¶„ì„

## ğŸ“ ìš”ì•½

### ì£¼ìš” ëª…ë ¹ì–´

```bash
# FRAMES ë‹¤ìš´ë¡œë“œ (ìë™ DB ë“±ë¡)
python backend/scripts/download_frames.py --fetch-wikipedia

# ëª¨ë“  BEIR ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (ìë™ DB ë“±ë¡)
python backend/scripts/prepare_dataset.py download-all

# ë“±ë¡ëœ ë°ì´í„°ì…‹ í™•ì¸
python backend/scripts/dataset_registry.py list

# ê¸°ì¡´ ë°ì´í„°ì…‹ ì¼ê´„ ë“±ë¡
python backend/scripts/dataset_registry.py auto-register
```

### ìë™ DB ë“±ë¡ ê¸°ëŠ¥ âœ¨

- ëª¨ë“  ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ëŠ” **ê¸°ë³¸ì ìœ¼ë¡œ ë°ì´í„°ì…‹ì„ DBì— ìë™ ë“±ë¡**í•©ë‹ˆë‹¤
- `--no-register` í”Œë˜ê·¸ë¡œ ìë™ ë“±ë¡ì„ ê±´ë„ˆë›¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì´ë¯¸ ë“±ë¡ëœ ë°ì´í„°ì…‹ì€ ìë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤
- `dataset_registry.py`ë¡œ ìˆ˜ë™ ê´€ë¦¬ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤

Happy Evaluating! ğŸš€

